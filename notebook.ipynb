{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co1z9vHFNkaA"
      },
      "source": [
        "# WiDS Datathon 2026 - TRACK 1\n",
        "Predicting Delayed Evacuation Alerts for Equitable Emergency Response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Definition\n",
        "\n",
        "Wildfires pose a rapidly evolving threat to communities, where timely evacuation alerts can be the difference between safety and severe harm.  \n",
        "However, evacuation alerts are not always issued promptly after a wildfire event is first reported. Delays in alert issuance may disproportionately affect vulnerable populations and reduce the effectiveness of emergency response.\n",
        "\n",
        "### Decision Context\n",
        "\n",
        "Emergency managers must decide **when** to issue evacuation alerts under uncertainty, often based on incomplete or evolving information.  \n",
        "This project aims to support that decision-making process by identifying situations where an evacuation alert is likely to be **issued late**, allowing authorities to intervene earlier and reduce risk.\n",
        "\n",
        "Rather than predicting wildfire severity, our focus is on **alert timeliness** — a critical but underexplored operational dimension of emergency management.\n"
      ],
      "metadata": {
        "id": "51MbrXpsQBNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selected Track: Accelerating Equitable Evacuations\n",
        "\n",
        "This project follows **Track 1: Accelerating Equitable Evacuations**.\n",
        "\n",
        "Our objective is to analyze and model delays in evacuation alerts following wildfire event reports, with the goal of:\n",
        "- identifying patterns associated with delayed alerts,\n",
        "- quantifying alert lag in a data-driven way,\n",
        "- and proposing a predictive framework that can flag high-risk situations early.\n",
        "\n",
        "The ultimate goal is not only predictive performance, but actionable insight for emergency response planning.\n"
      ],
      "metadata": {
        "id": "dVV8dUm7QKQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Sources\n",
        "\n",
        "We use the official **WiDS Datathon WatchDuty wildfire dataset**, which integrates wildfire events, evacuation zones, and detailed change logs.\n",
        "\n",
        "The key data sources used in this project are:\n",
        "\n",
        "- **Geo Events (`geo_events_geoevent.csv`)**  \n",
        "  Records wildfire-related events and incidents.\n",
        "\n",
        "- **Geo Event Change Log (`geo_events_geoeventchangelog.csv`)**  \n",
        "  Tracks updates and modifications to wildfire events over time.  \n",
        "  This dataset is critical for identifying the earliest moment an event was reported.\n",
        "\n",
        "- **Evacuation Zones (`evac_zones_gis_evaczone.csv`)**  \n",
        "  Contains evacuation zone declarations, including the timestamp at which alerts become effective.\n",
        "\n",
        "- **Evacuation Zone – Geo Event Mapping (`evac_zone_status_geo_event_map.csv`)**  \n",
        "  Links wildfire events to affected evacuation zones.\n",
        "\n",
        "These datasets allow us to reconstruct the timeline between the first report of a wildfire event and the issuance of evacuation alerts.\n"
      ],
      "metadata": {
        "id": "ONxVBbwGQQP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analytical Unit\n",
        "\n",
        "The primary unit of analysis in this project is a **(wildfire event × evacuation zone)** pair.\n",
        "\n",
        "This choice reflects how evacuation decisions are made in practice:\n",
        "- a single wildfire event may affect multiple evacuation zones,\n",
        "- and each zone may receive alerts at different times.\n",
        "\n",
        "### Target Variable: Delayed Evacuation Alert\n",
        "\n",
        "To support decision-making, we define a binary target variable:\n",
        "\n",
        "**Late Evacuation Alert**\n",
        "\n",
        "An alert is considered *late* if there is a substantial delay between:\n",
        "- the first recorded report of a wildfire event, and\n",
        "- the time at which an evacuation alert becomes effective for a given zone.\n",
        "\n",
        "This formulation allows us to frame the problem as a classification task, where the goal is to identify high-risk situations for delayed alerts.\n"
      ],
      "metadata": {
        "id": "HVtlFWvuQSA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methodological Approach\n",
        "\n",
        "Our analysis proceeds in the following steps:\n",
        "\n",
        "1. Identify the earliest reported timestamp for each wildfire event using change logs.\n",
        "2. Extract evacuation alert effective times for each evacuation zone.\n",
        "3. Compute the time delay (\"alert lag\") between event reporting and alert issuance.\n",
        "4. Define a data-driven threshold to label alerts as delayed or on-time.\n",
        "5. Explore patterns and drivers of delayed alerts through exploratory data analysis.\n",
        "6. Build an interpretable classification model to flag high-risk cases.\n",
        "7. Translate model outputs into a decision-support framework.\n",
        "\n",
        "This structured approach ensures transparency, reproducibility, and relevance for real-world use.\n"
      ],
      "metadata": {
        "id": "C3euP68wQWw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technical Setup\n",
        "\n",
        "All analyses are conducted in Google Colab to ensure reproducibility and alignment with WiDS Datathon guidelines.\n",
        "\n",
        "Data files are downloaded programmatically using the Kaggle API and are not stored in this repository.  \n",
        "This ensures that the notebook can be rerun end-to-end by reviewers without manual data handling.\n"
      ],
      "metadata": {
        "id": "R0pAZXWrQasb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u2Uc9UMGNkaA",
        "outputId": "72ccc93f-91ea-41ce-acc8-392ef6318c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4bee4361-caae-40ab-bb85-78d33c964805\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4bee4361-caae-40ab-bb85-78d33c964805\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"leonardomarcu\",\"key\":\"90e4f9e2c12bb3de5825ba1691c7ad2d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1) Kaggle download (Colab)\n",
        "# ===============================\n",
        "!pip -q install kaggle\n",
        "\n",
        "import os, zipfile, glob\n",
        "\n",
        "# If you already uploaded kaggle.json earlier, keep it in the current folder.\n",
        "assert os.path.exists(\"kaggle.json\"), \"Upload kaggle.json in the Colab session first.\"\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download competition data\n",
        "COMP = \"wids-university-datathon-2025\"   # <-- if this errors, we will adjust the slug\n",
        "!kaggle competitions download -c {COMP}\n",
        "\n",
        "# Unzip into /content/data\n",
        "!mkdir -p data\n",
        "zip_path = f\"{COMP}.zip\"\n",
        "assert os.path.exists(zip_path), f\"Expected {zip_path} after download.\"\n",
        "!unzip -o {zip_path} -d data\n"
      ],
      "metadata": {
        "id": "F3OQvAN-N9qp",
        "outputId": "d72f5d9c-ac24-49ac-8699-01c42055b310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wids-university-datathon-2025.zip to /content\n",
            " 92% 372M/403M [00:00<00:00, 658MB/s] \n",
            "100% 403M/403M [00:00<00:00, 638MB/s]\n",
            "Archive:  wids-university-datathon-2025.zip\n",
            "  inflating: data/WiDS _-_ Watch Duty_ Data Dictionary.docx  \n",
            "  inflating: data/evac_zone_status_geo_event_map.csv  \n",
            "  inflating: data/evac_zones_gis_evaczone.csv  \n",
            "  inflating: data/evac_zones_gis_evaczonechangelog.csv  \n",
            "  inflating: data/fire_perimeters_gis_fireperimeter.csv  \n",
            "  inflating: data/fire_perimeters_gis_fireperimeterchangelog.csv  \n",
            "  inflating: data/geo_events_externalgeoevent.csv  \n",
            "  inflating: data/geo_events_externalgeoeventchangelog.csv  \n",
            "  inflating: data/geo_events_geoevent.csv  \n",
            "  inflating: data/geo_events_geoeventchangelog.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "print(\"Top-level in data/:\", os.listdir(\"data\"))\n",
        "\n",
        "all_files = sorted(glob.glob(\"data/**/*\", recursive=True))\n",
        "print(\"Num files:\", len(all_files))\n",
        "for f in all_files[:120]:\n",
        "    print(f)\n"
      ],
      "metadata": {
        "id": "RkATWkarPPEH",
        "outputId": "5dea4387-7d96-4ad9-f8f9-23c23ace6f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-level in data/: ['geo_events_geoevent.csv', 'evac_zones_gis_evaczonechangelog.csv', 'geo_events_geoeventchangelog.csv', 'fire_perimeters_gis_fireperimeterchangelog.csv', 'WiDS _-_ Watch Duty_ Data Dictionary.docx', 'fire_perimeters_gis_fireperimeter.csv', 'geo_events_externalgeoeventchangelog.csv', 'evac_zone_status_geo_event_map.csv', 'evac_zones_gis_evaczone.csv', 'geo_events_externalgeoevent.csv']\n",
            "Num files: 10\n",
            "data/WiDS _-_ Watch Duty_ Data Dictionary.docx\n",
            "data/evac_zone_status_geo_event_map.csv\n",
            "data/evac_zones_gis_evaczone.csv\n",
            "data/evac_zones_gis_evaczonechangelog.csv\n",
            "data/fire_perimeters_gis_fireperimeter.csv\n",
            "data/fire_perimeters_gis_fireperimeterchangelog.csv\n",
            "data/geo_events_externalgeoevent.csv\n",
            "data/geo_events_externalgeoeventchangelog.csv\n",
            "data/geo_events_geoevent.csv\n",
            "data/geo_events_geoeventchangelog.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}